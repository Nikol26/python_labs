# –õ–†3 ‚Äî –¢–µ–∫—Å—Ç—ã –∏ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ (—Å–ª–æ–≤–∞—Ä—å/–º–Ω–æ–∂–µ—Å—Ç–≤–æ)

# –ó–∞–¥–∞–Ω–∏–µ A ‚Äî src/lib/text.py
## normalize
<pre><code>
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    text = text.casefold()
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    text = text.replace('\t', ' ').replace('\r', ' ').replace('\n', ' ')
    text = ' '.join(text.split())
    text = text.strip()
    return text
print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")) 
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))
</code></pre>
<img width="515" height="308" alt="image" src="https://github.com/user-attachments/assets/0d863029-b948-4795-acee-6bdbd9582911" />

## tokenize 
<pre><code>
import re 
def tokenize(text: str) -> list[str]:
    return re.findall(r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å]+', text)
print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
</code></pre>
<img width="400" height="296" alt="image" src="https://github.com/user-attachments/assets/dd3b56d6-7c82-4260-b2b5-b8414b0d4e79" />

## count_freq + top_n
<pre><code>
def count_freq(tokens: list[str]) -> dict[str, int]:
    counts = {}  
    for word in tokens:
        current_count = counts.get(word, 0)
        counts[word] = current_count + 1
    return counts
def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    temp_list = []
    for word, count in freq.items():
        temp_list.append((-count, word))
    temp_list.sort()
    result = []
    for neg_count, word in temp_list:
        result.append((word, -neg_count))

    return result[:n]
tokens_example = ["a", "b", "a", "c", "b", "a"]
freq_example = count_freq(tokens_example)
print(top_n(freq_example, n=2))
tokens_example_2 = ["bb", "aa", "bb", "aa", "cc"]
freq_example_2 = count_freq(tokens_example_2)
print(top_n(freq_example_2, n=2))
</code></pre>
<img width="490" height="435" alt="image" src="https://github.com/user-attachments/assets/1e2fb9e0-67c7-4fbd-87d6-cb66db251c8b" />

# –ó–∞–¥–∞–Ω–∏–µ B ‚Äî src/text_stats.py (—Å–∫—Ä–∏–ø—Ç —Å–æ stdin)
<pre><code>
import sys
sys.path.append(r'C:\Users\VektorVkusoff\.vscode\python_labs\src')

from text import normalize, tokenize, count_freq, top_n

def table(arr: list[tuple[str, int]], isTable: bool = True) -> str:
    if not arr:
        return "(–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)"
    s = str()
    if isTable:
        word_col_width = max(len("—Å–ª–æ–≤–æ"), max(len(a[0]) for a in arr))
        freq_col_width = max(len("—á–∞—Å—Ç–æ—Ç–∞"), max(len(str(a[1])) for a in arr))
        s += f"{'—Å–ª–æ–≤–æ'.ljust(word_col_width)} | {'—á–∞—Å—Ç–æ—Ç–∞'.rjust(freq_col_width)}"
        s += "\n" + "-" * word_col_width + "-+-" + "-" * freq_col_width
        for word, freq in arr:
            s += f"\n{word.ljust(word_col_width)} | {str(freq).rjust(freq_col_width)}"
        return s
    else:
        return "\n".join(f"{a[0]}: {a[1]}" for a in arr)
def main(text: str):
    text = text.strip()
    tokens = normalize(text)
    tokens = tokenize(tokens)
    freqs = count_freq(tokens)
    total_words = len(tokens)
    unique_words = len(freqs)
    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
    top5 = sorted(freqs.items(), key=lambda x: x[1], reverse=True)[:5]
    print("–¢–æ–ø-5:")
    print(table(top5, True))
</code></pre>
<img width="569" height="460" alt="image" src="https://github.com/user-attachments/assets/8edfb127-8cc7-42b3-8bec-90574d9ec22e" />

    
    




  


  

  
